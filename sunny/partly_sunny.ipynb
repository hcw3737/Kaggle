{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/crowdflower-weather-twitter/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/crowdflower-weather-twitter/test.csv\")\nall_data = pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 1000\nall_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variable = pd.read_table(\"/kaggle/input/crowdflower-weather-twitter/variableNames.txt\",header=None)\nvariable\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = \"/kaggle/input/crowdflower-weather-twitter/variableNames.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(var) as f:    #open - 파일 입출력?\n    print(f.read())\n# https://dojang.io/mod/page/view.php?id=2325","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.loc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(20):\n    print(i, ' : ', all_data['tweet'].iloc[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data.fillna(\"nan\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[\"tweet\"] = all_data[\"tweet\"] + \" \" + all_data[\"state\"] + \" \" + all_data[\"location\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[\"tweet\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer   #count를 기반으로..\ntfidf = TfidfVectorizer(min_df=4,ngram_range=(1, 2),sublinear_tf=True)\ntext = tfidf.fit_transform(all_data[\"tweet\"])\n# min_df=1(최소단어등장횟수) 최소3은 넣어주자\n# ngram_range=(1, 1), 윈도우사이즈 (conv1느낌) 한번에 단어를 인식을할때 몇개씩 인식할거냐\n# sublinear_tf=False 칼럼마다 스케일링해주는것","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#트리모델 안쓰고 선형모델,딥러닝 쓸거기때문에 차원축소(svd.. 이런거) 따로 안해도됨.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text #120103:데이터개수 / 74398: 단어의개수(칼럼의개수) / 3234262: 0101애들중에 1인 애들의 개수\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = text[:len(train)]\ntest_text = text[len(train):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge  \n#예측을 한 컬럼만 해주는게 아니라 여러개의 칼럼을 동시 예측해주는데, regression을 여러번 실행하는데 도움을 주는\nfrom sklearn.multioutput import RegressorChain\nrc = RegressorChain(Ridge(alpha=5, random_state=1234),random_state=1234) \n#alpha(규제항-과적합 예방) 디폴트는 1\nrc.fit(train_text,train.iloc[:,4:])  #train에서 \n\nresult = rc.predict(test_text)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#result값을 보면 <0, 1< 인 값들이 존재하는데, 우리는 0~1사이의 값을 예측해야하기 때문에 (0,1)값으로 변환해준다.\n#softmax를 적용해봐도 되지 않았을까...?(0~1사이값으로 만들어주니까..?)\nresult = np.where(result < 0, 0, result)  \nresult = np.where(result > 1, 1, result)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/crowdflower-weather-twitter/sampleSubmission.csv\")\nsub.iloc[:,1:] = result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"############# 여기부턴 딥러닝 ###############"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 토크나이저 선언\nfrom keras.preprocessing.text import Tokenizer\ntk = Tokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 정수인코딩\n# 코퍼스의 모든 단어를 숫자로 매핑\ntk.fit_on_texts(all_data[\"tweet\"])\ntk.word_index # 빈도순 # 빈도순","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tk.word_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tk.word_index) # 단어의 총 개수(종류 개수)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = tk.texts_to_sequences(all_data[\"tweet\"]) # 각 텍스트를 숫자로\nall_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nall_pad = pad_sequences(all_text,)  #문장 길이 모두 맞춰주기 위해 '0'으로 채워준다.\nall_pad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pad.shape #가장 길었던 애가 단어 48개","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pad = all_pad[:len(train)]\ntest_pad = all_pad[len(train):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"veec = pd.read_table(\"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"veec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pre-trained된 embedding을 사용해볼 것이다.\n#가져온 벡터를 dict형태로 바꿔주는 함수\ndef load_embedding(path):\n    embeddings = {}\n    with open(path) as f:\n        for i in f:\n            value = i.rstrip().split() #rstrip 맨 오른쪽 띄어쓰기 지워줌\n            word = value[0]\n#             vector = value[1:] #이게 램 많이 잡아먹는코드 #그냥하면 2백만개가져와서 ram터짐\n            vector = np.asarray(value[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings1 = load_embedding(\"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tk.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 가져온 pre-trained 임베딩 전체 단어 중 여기 코퍼스에 있는 단어만 뽑아와주는 함수.\ndef filter_embedding(embeddings, word_index, vocab_size, dim):\n    embedding_matrix = np.zeros([vocab_size,dim])  #초기화 -초기값으로 설정(초기값='0')\n    for word, i in word_index.items():\n        vector = embeddings.get(word)  \n        # embeddings : 우리가 가져온 pretrained embedding\n        # embedding에 사용된 대용량의 word들을 가져와 저장(리스트 형태로)\n        if vector is not None:\n            embedding_matrix[i] = vector\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = filter_embedding(embeddings1, tk.word_index, len(tk.word_index)+1,300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Sequential\nfrom keras.layers import *\nmodel = Sequential()\n# model.add(Embedding(len(tk.word_index)+1,10,input_length=all_pad.shape[1]))\nmodel.add(Embedding(len(tk.word_index)+1,300,input_length=all_pad.shape[1],weights=[embedding_matrix],trainable=False))\n# weights 에 pretrained embedding으로 얻은 embedding_matrix로 사용\n# trainable = False : pre-trained 모델 학습은 실행하지 않도록 fix해놓는것\nmodel.add(GRU(128)) #LSTM의 단점을 보완하고 속도를 향상시킨 방법론 GRU\nmodel.add(Dense(64,activation=\"relu\"))\n\n#출력층\nmodel.add(Dense(24)) #예측해야할것 column 개수가 24개, #activation의 디폴트는 linear임\nmodel.compile(optimizer=\"adam\",\n              loss=\"mse\") # 이번엔 classification이 아니라 regression이기 때문에 loss function은 mse로\nmodel.fit(train_pad,train.iloc[:,4:],batch_size=64,epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_text = model.predict(test_pad)\nresult_text = np.where(result_text < 0, 0, result_text)\nresult_text = np.where(result_text > 1, 1, result_text)\n# result_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/crowdflower-weather-twitter/sampleSubmission.csv\")\nsub.iloc[:,1:] = result_text\n# 앙상블\nsub.iloc[:,1:] = 0.8*result_text + 0.2*result\nsub.to_csv(\"submission_ensemble.csv\",index = 0)\n\n# pre-trained embedding관련 참고하면 매우 유용\n# https://dacon.io/competitions/official/235670/codeshare/1892?dtype=recent","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}